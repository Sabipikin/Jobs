{
  "paragraphs": [
    {
      "text": "// Import Air Pollution Data\r\nval airPollutionData = spark.read.option(\"header\", \"true\").csv(\"hdfs:///kiki/air_pollution_exp_by_country_oecd.csv\")\r\n\r\n// Display schema and sample data\r\nairPollutionData.printSchema()\r\nairPollutionData.show(5)",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:42:45+0000",
      "progress": 100,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- LOCATION: string (nullable = true)\n |-- INDICATOR: string (nullable = true)\n |-- SUBJECT: string (nullable = true)\n |-- MEASURE: string (nullable = true)\n |-- FREQUENCY: string (nullable = true)\n |-- TIME: string (nullable = true)\n |-- Value: string (nullable = true)\n |-- Flag Codes: string (nullable = true)\n\n+---------+------------+----------+---------+---------+----+--------+----------+\n| LOCATION|   INDICATOR|   SUBJECT|  MEASURE|FREQUENCY|TIME|   Value|Flag Codes|\n+---------+------------+----------+---------+---------+----+--------+----------+\n|Australia|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|2010|10.53559|      null|\n|Australia|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|2011|10.88267|      null|\n|Australia|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|2012|10.40148|      null|\n|Australia|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|2013| 9.84329|      null|\n|Australia|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|2014| 9.36117|      null|\n+---------+------------+----------+---------+---------+----+--------+----------+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mairPollutionData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, INDICATOR: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=0",
              "$$hashKey": "object:44205"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=1",
              "$$hashKey": "object:44206"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690528963825_121288823",
      "id": "paragraph_1690528963825_121288823",
      "dateCreated": "2023-07-28T07:22:43+0000",
      "dateStarted": "2023-07-28T15:42:45+0000",
      "dateFinished": "2023-07-28T15:44:21+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:43919"
    },
    {
      "text": "// Check for missing data\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\nairPollutionData.select(airPollutionData.columns.map(colName => {\r\n  count(when(col(colName).isNull, true)).alias(colName)\r\n}): _*).show()\r\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:45:03+0000",
      "progress": 50,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 135.969,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+---------+-------+-------+---------+----+-----+----------+\n|LOCATION|INDICATOR|SUBJECT|MEASURE|FREQUENCY|TIME|Value|Flag Codes|\n+--------+---------+-------+-------+---------+----+-----+----------+\n|       0|        0|      0|      0|        0|   0|    0|       288|\n+--------+---------+-------+-------+---------+----+-----+----------+\n\nimport org.apache.spark.sql.functions._\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=2",
              "$$hashKey": "object:44804"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=3",
              "$$hashKey": "object:44805"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690535263846_1593372554",
      "id": "paragraph_1690535263846_1593372554",
      "dateCreated": "2023-07-28T09:07:43+0000",
      "dateStarted": "2023-07-28T15:45:03+0000",
      "dateFinished": "2023-07-28T15:45:08+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43920"
    },
    {
      "text": "// Drop Flag Cost column due to null values\nval airPollutionDataWithoutNullColumns = airPollutionData.na.drop(Seq(\"Flag Codes\"))",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:45:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mairPollutionDataWithoutNullColumns\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, INDICATOR: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690543147508_1659507265",
      "id": "paragraph_1690543147508_1659507265",
      "dateCreated": "2023-07-28T11:19:07+0000",
      "dateStarted": "2023-07-28T15:45:20+0000",
      "dateFinished": "2023-07-28T15:45:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43921"
    },
    {
      "text": "// Check for dublicates\n\nprintln(\"Count of rows: \" + airPollutionData.count())\nprintln(\"Count of distinct rows: \" + airPollutionData.distinct().count())",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:45:29+0000",
      "progress": 50,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Count of rows: 288\nCount of distinct rows: 288\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=4",
              "$$hashKey": "object:44949"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=5",
              "$$hashKey": "object:44950"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=6",
              "$$hashKey": "object:44951"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=7",
              "$$hashKey": "object:44952"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=8",
              "$$hashKey": "object:44953"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690535322370_42861443",
      "id": "paragraph_1690535322370_42861443",
      "dateCreated": "2023-07-28T09:08:42+0000",
      "dateStarted": "2023-07-28T15:45:29+0000",
      "dateFinished": "2023-07-28T15:45:34+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43922"
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=9",
              "$$hashKey": "object:45080"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=10",
              "$$hashKey": "object:45081"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690559212650_1030135664",
      "id": "paragraph_1690559212650_1030135664",
      "dateCreated": "2023-07-28T15:46:52+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:44964",
      "text": "// Data Description\n\nairPollutionData.describe().show()",
      "dateUpdated": "2023-07-28T15:47:26+0000",
      "dateFinished": "2023-07-28T15:47:32+0000",
      "dateStarted": "2023-07-28T15:47:26+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+-------------+------------+----------+---------+---------+-----------------+------------------+----------+\n|summary|     LOCATION|   INDICATOR|   SUBJECT|  MEASURE|FREQUENCY|             TIME|             Value|Flag Codes|\n+-------+-------------+------------+----------+---------+---------+-----------------+------------------+----------+\n|  count|          288|         288|       288|      288|      288|              288|               288|         0|\n|   mean|         null|        null|      null|     null|     null|           2013.5|14.040981076388885|      null|\n| stddev|         null|        null|      null|     null|     null|2.295276167028016| 5.863410675311603|      null|\n|    min|    Australia|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|             2010|          10.01998|      null|\n|    max|United States|POLLUTIONEXP|EXPOS2PM25|MICGRCUBM|        A|             2017|           9.91419|      null|\n+-------+-------------+------------+----------+---------+---------+-----------------+------------------+----------+\n\n"
          }
        ]
      }
    },
    {
      "text": "// Trend Analysis\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\n// Group by LOCATION and TIME and calculate the mean pollution value for each year\r\nval trendAnalysisData = airPollutionData\r\n  .groupBy(\"LOCATION\", \"TIME\")\r\n  .agg(avg(\"Value\").as(\"MeanPollution\"))\r\n  .orderBy(\"LOCATION\", \"TIME\")\r\n\r\n// Show the trend analysis data\r\ntrendAnalysisData.show()\r\n\r\n// Identify countries or regions with increasing or decreasing pollution levels\r\nval pollutionTrendByLocation = trendAnalysisData\r\n  .groupBy(\"LOCATION\")\r\n  .agg(first(\"MeanPollution\").as(\"StartValue\"), last(\"MeanPollution\").as(\"EndValue\"))\r\n  .withColumn(\"Trend\", when(col(\"EndValue\") > col(\"StartValue\"), \"Increasing\")\r\n                      .when(col(\"EndValue\") < col(\"StartValue\"), \"Decreasing\")\r\n                      .otherwise(\"Stable\"))\r\n\r\n// Show the pollution trend by location\r\npollutionTrendByLocation.show()",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:47:56+0000",
      "progress": 33,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------+----+-------------+\n| LOCATION|TIME|MeanPollution|\n+---------+----+-------------+\n|Australia|2010|     10.53559|\n|Australia|2011|     10.88267|\n|Australia|2012|     10.40148|\n|Australia|2013|      9.84329|\n|Australia|2014|      9.36117|\n|Australia|2015|      9.20277|\n|Australia|2016|      8.53581|\n|Australia|2017|      8.52437|\n|  Austria|2010|     15.36477|\n|  Austria|2011|     15.67401|\n|  Austria|2012|     14.50341|\n|  Austria|2013|     14.28511|\n|  Austria|2014|     13.53498|\n|  Austria|2015|     13.59913|\n|  Austria|2016|     12.65744|\n|  Austria|2017|     12.68774|\n|  Belgium|2010|     16.22427|\n|  Belgium|2011|     16.01357|\n|  Belgium|2012|     14.93425|\n|  Belgium|2013|     14.29062|\n+---------+----+-------------+\nonly showing top 20 rows\n\n+-------------+----------+--------+----------+\n|     LOCATION|StartValue|EndValue|     Trend|\n+-------------+----------+--------+----------+\n|       Sweden|   7.21877| 6.13443|Decreasing|\n|      Germany|  15.38112|12.08939|Decreasing|\n|       France|  14.84296|11.95962|Decreasing|\n|       Greece|  19.13968|16.47663|Decreasing|\n|      Belgium|  16.22427|13.09935|Decreasing|\n|      Finland|   7.28899| 5.90603|Decreasing|\n|United States|   9.29297| 7.36365|Decreasing|\n|        Chile|  27.19477|22.13709|Decreasing|\n|        Italy|  18.99694| 16.4971|Decreasing|\n|    Lithuania|  14.97421|11.90224|Decreasing|\n|       Norway|   8.49182|  7.0458|Decreasing|\n|        Spain|  11.72598| 9.91419|Decreasing|\n|      Denmark|  12.30225|10.35431|Decreasing|\n|      Ireland|  10.01998| 8.27177|Decreasing|\n|      Iceland|   8.21186| 6.78283|Decreasing|\n|       Israel|  23.78346| 20.8113|Decreasing|\n|       Mexico|  26.75453|21.23615|Decreasing|\n|      Estonia|   8.64197| 6.84199|Decreasing|\n|  Switzerland|  13.09833|10.44407|Decreasing|\n|       Latvia|  17.91547|14.10963|Decreasing|\n+-------------+----------+--------+----------+\nonly showing top 20 rows\n\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mtrendAnalysisData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [LOCATION: string, TIME: string ... 1 more field]\n\u001b[1m\u001b[34mpollutionTrendByLocation\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, StartValue: double ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=11",
              "$$hashKey": "object:45270"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=12",
              "$$hashKey": "object:45271"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=13",
              "$$hashKey": "object:45272"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=14",
              "$$hashKey": "object:45273"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=15",
              "$$hashKey": "object:45274"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=16",
              "$$hashKey": "object:45275"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=17",
              "$$hashKey": "object:45276"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690535844071_935359912",
      "id": "paragraph_1690535844071_935359912",
      "dateCreated": "2023-07-28T09:17:24+0000",
      "dateStarted": "2023-07-28T15:47:56+0000",
      "dateFinished": "2023-07-28T15:48:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43923"
    },
    {
      "text": "// Geograhical Analysis\r\n\r\nimport org.apache.spark.sql.expressions.Window\r\nimport org.apache.spark.sql.types.DoubleType\r\n\r\n// Assuming you have 'geoData' DataFrame containing 'LOCATION' and 'GeoCode' columns\r\n// If you have the actual geospatial data, you can load it using appropriate libraries.\r\n\r\n// Group by LOCATION and calculate the mean pollution value for each country or region\r\nval geographicAnalysisData = airPollutionData\r\n  .groupBy(\"LOCATION\")\r\n  .agg(avg(\"Value\").cast(DoubleType).as(\"MeanPollution\"))\r\n\r\n// Calculate the global rank based on mean pollution value (1 for the lowest pollution)\r\nval pollutionRankWindowSpec = Window.orderBy(\"MeanPollution\")\r\nval geographicAnalysisRanked = geographicAnalysisData\r\n  .withColumn(\"PollutionRank\", rank().over(pollutionRankWindowSpec))\r\n\r\n// Take the top 20 countries (ranked from 1 to 20) based on pollution levels\r\nval top20Countries = geographicAnalysisRanked.filter(col(\"PollutionRank\") <= 20)\r\n\r\n// Show the final table with the top 20 countries\r\ntop20Countries.show()",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:48:14+0000",
      "progress": 50,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------+------------------+-------------+\n|      LOCATION|     MeanPollution|PollutionRank|\n+--------------+------------------+-------------+\n|       Finland|        6.53085875|            1|\n|        Sweden|           6.64313|            2|\n|   New Zealand| 6.670280000000001|            3|\n|       Iceland|         7.4987175|            4|\n|        Canada| 7.526947500000001|            5|\n|       Estonia|         7.6526525|            6|\n|        Norway|        7.81130625|            7|\n| United States|        8.44010875|            8|\n|      Portugal|           9.12072|            9|\n|       Ireland|           9.17984|           10|\n|     Australia|        9.66089375|           11|\n|         Spain|10.901471249999998|           12|\n|    Luxembourg|        11.2949225|           13|\n|       Denmark|       11.32205625|           14|\n|United Kingdom|        11.3766775|           15|\n|   Switzerland|       11.85036875|           16|\n|         Japan|13.126637500000001|           17|\n|     Lithuania|       13.18664625|           18|\n|   Netherlands|       13.33380625|           19|\n|        France|13.371120000000001|           20|\n+--------------+------------------+-------------+\n\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.types.DoubleType\n\u001b[1m\u001b[34mgeographicAnalysisData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, MeanPollution: double]\n\u001b[1m\u001b[34mpollutionRankWindowSpec\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.WindowSpec\u001b[0m = org.apache.spark.sql.expressions.WindowSpec@6e9db520\n\u001b[1m\u001b[34mgeographicAnalysisRanked\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, MeanPollution: double ... 1 more field]\n\u001b[1m\u001b[34mtop20Countries\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [LOCATION: string, MeanPollution: double ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=18",
              "$$hashKey": "object:45374"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=19",
              "$$hashKey": "object:45375"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=20",
              "$$hashKey": "object:45376"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690532718158_102969837",
      "id": "paragraph_1690532718158_102969837",
      "dateCreated": "2023-07-28T08:25:18+0000",
      "dateStarted": "2023-07-28T15:48:14+0000",
      "dateFinished": "2023-07-28T15:48:18+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43924"
    },
    {
      "text": "// pollution measures\r\n\r\nimport org.apache.spark.sql.{SparkSession, DataFrame}\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types.DoubleType\r\n\r\n// Assuming you have already loaded and preprocessed the air pollution data into a DataFrame named 'airPollutionData'\r\n\r\n// Step 1: Convert 'Value' column to double\r\nval airPollutionDataCleaned: DataFrame = airPollutionData.withColumn(\"Value\", col(\"Value\").cast(DoubleType))\r\n\r\n// Step 2: Group data by 'MEASURE' and calculate statistics\r\nval pollutionMeasureStats: DataFrame = airPollutionDataCleaned\r\n  .groupBy(\"MEASURE\")\r\n  .agg(\r\n    avg(\"Value\").alias(\"Average_Pollution_Level\"),\r\n    min(\"Value\").alias(\"Min_Pollution_Level\"),\r\n    max(\"Value\").alias(\"Max_Pollution_Level\"),\r\n    stddev(\"Value\").alias(\"Pollution_Standard_Deviation\")\r\n  )\r\n  .orderBy(\"MEASURE\")\r\n\r\n// Step 3: Show the results\r\npollutionMeasureStats.show()",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T15:52:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---------+-----------------------+-------------------+-------------------+----------------------------+\n|  MEASURE|Average_Pollution_Level|Min_Pollution_Level|Max_Pollution_Level|Pollution_Standard_Deviation|\n+---------+-----------------------+-------------------+-------------------+----------------------------+\n|MICGRCUBM|     14.040981076388885|            5.90603|            30.0354|           5.863410675311603|\n+---------+-----------------------+-------------------+-------------------+----------------------------+\n\nimport org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.DoubleType\n\u001b[1m\u001b[34mairPollutionDataCleaned\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, INDICATOR: string ... 6 more fields]\n\u001b[1m\u001b[34mpollutionMeasureStats\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [MEASURE: string, Average_Pollution_Level: double ... 3 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=21",
              "$$hashKey": "object:45474"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=22",
              "$$hashKey": "object:45475"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690536547726_328890732",
      "id": "paragraph_1690536547726_328890732",
      "dateCreated": "2023-07-28T09:29:07+0000",
      "dateStarted": "2023-07-28T15:52:33+0000",
      "dateFinished": "2023-07-28T15:52:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43925"
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=23",
              "$$hashKey": "object:45606"
            },
            {
              "jobUrl": "http://cluster-faba-m-0.europe-central2-c.c.precise-datum-394114.internal:38113/jobs/job?id=24",
              "$$hashKey": "object:45607"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690559676828_1140945264",
      "id": "paragraph_1690559676828_1140945264",
      "dateCreated": "2023-07-28T15:54:36+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:45484",
      "text": "// Pollution Level Based On Indicators\r\n\r\nimport org.apache.spark.sql.{SparkSession, DataFrame}\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types.DoubleType\r\n\r\n// Assuming you have already loaded and preprocessed the air pollution data into a DataFrame named 'airPollutionData'\r\n\r\n// Step 1: Convert 'Value' column to double\r\nval airPollutionDataCleaned: DataFrame = airPollutionData.withColumn(\"Value\", col(\"Value\").cast(DoubleType))\r\n\r\n// Step 2: Group data by 'INDICATOR' and 'SUBJECT' and calculate statistics\r\nval pollutionIndicatorSubjectStats: DataFrame = airPollutionDataCleaned\r\n  .groupBy(\"INDICATOR\", \"SUBJECT\")\r\n  .agg(\r\n    avg(\"Value\").alias(\"Average_Pollution_Level\"),\r\n    min(\"Value\").alias(\"Min_Pollution_Level\"),\r\n    max(\"Value\").alias(\"Max_Pollution_Level\"),\r\n    stddev(\"Value\").alias(\"Pollution_Standard_Deviation\")\r\n  )\r\n  .orderBy(\"INDICATOR\", \"SUBJECT\")\r\n\r\n// Step 3: Show the results\r\npollutionIndicatorSubjectStats.show()\r\n",
      "dateUpdated": "2023-07-28T15:55:23+0000",
      "dateFinished": "2023-07-28T15:55:26+0000",
      "dateStarted": "2023-07-28T15:55:23+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------+----------+-----------------------+-------------------+-------------------+----------------------------+\n|   INDICATOR|   SUBJECT|Average_Pollution_Level|Min_Pollution_Level|Max_Pollution_Level|Pollution_Standard_Deviation|\n+------------+----------+-----------------------+-------------------+-------------------+----------------------------+\n|POLLUTIONEXP|EXPOS2PM25|     14.040981076388885|            5.90603|            30.0354|           5.863410675311603|\n+------------+----------+-----------------------+-------------------+-------------------+----------------------------+\n\nimport org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.DoubleType\n\u001b[1m\u001b[34mairPollutionDataCleaned\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [LOCATION: string, INDICATOR: string ... 6 more fields]\n\u001b[1m\u001b[34mpollutionIndicatorSubjectStats\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [INDICATOR: string, SUBJECT: string ... 4 more fields]\n"
          }
        ]
      }
    },
    {
      "text": "// Visualization Using Scala",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T12:12:41+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690539006686_581480349",
      "id": "paragraph_1690539006686_581480349",
      "dateCreated": "2023-07-28T10:10:06+0000",
      "dateStarted": "2023-07-28T12:12:42+0000",
      "dateFinished": "2023-07-28T12:12:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43926"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2023-07-28T14:10:25+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "completionKey": "TAB",
          "editOnDblClick": false,
          "completionSupport": true,
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690538208337_529190961",
      "id": "paragraph_1690538208337_529190961",
      "dateCreated": "2023-07-28T09:56:48+0000",
      "status": "FINISHED",
      "$$hashKey": "object:43927"
    }
  ],
  "name": "Kiki",
  "id": "2J7HZTW23",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "simple",
    "personalizedMode": "false"
  },
  "info": {
    "isRunning": false
  },
  "path": "/Kiki"
}